{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Transcription using Whisper in Google Colab\n",
    "\n",
    "This notebook needs 2 folders in the root of your Google Drive in order to work:\n",
    "- lecture_transcriptions\n",
    "- lecture_audio_files\n",
    "\n",
    "Fill them with your desired audio files and lecture_transcriptions will start to fill up.\n",
    "\n",
    "As a solution to Google Colab timing out for free users the code checks if there is an existing transcription with the same name before transcribing. That way you can always press ***Runtime -> Run All*** without a worry."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import files\n",
    "from datetime import timedelta\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "import tqdm\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "AYhQrO1h-UQw"
   },
   "outputs": [],
   "source": [
    "def save_transcription(whisper_result, transcription_name):\n",
    "  first_line = True\n",
    "\n",
    "  with open(f\"drive/MyDrive/lecture_transcriptions/{transcription_name}.csv\", \"w\") as f:\n",
    "    if first_line:\n",
    "      first_line = False\n",
    "      f.write('Speech;Timestamp\\n')\n",
    "      \n",
    "    for segment in whisper_result[\"segments\"]:\n",
    "      speech_id = segment[\"id\"]\n",
    "      speech_text = segment[\"text\"]\n",
    "      start = segment[\"start\"]\n",
    "      end = segment[\"end\"]\n",
    "\n",
    "      speech_time = f\"{str(timedelta(seconds = start))} - {str(timedelta(seconds = end))}\"\n",
    "      text_line = f\"{speech_text};{speech_time}\\n\".lstrip()\n",
    "      f.write(text_line)\n",
    "\n",
    "  with open(f\"drive/MyDrive/lecture_transcriptions/{transcription_name}.txt\", \"w\") as f:\n",
    "    f.write(whisper_result[\"text\"])\n",
    "\n",
    "  # files.download(f\"drive/MyDrive/lecture_transcriptions/{transcription_name}.tsv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kosakhNmxb7A"
   },
   "source": [
    "## Install Whisper from github"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZsJUxc0aRsAf",
    "outputId": "3178afbb-9f13-4b6c-cc0a-8e49ce47775b"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "  Installing build dependencies ... \u001B[?25l\u001B[?25hdone\n",
      "  Getting requirements to build wheel ... \u001B[?25l\u001B[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001B[?25l\u001B[?25hdone\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m63.3/63.3 MB\u001B[0m \u001B[31m13.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.7/1.7 MB\u001B[0m \u001B[31m67.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m145.0/145.0 KB\u001B[0m \u001B[31m15.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25h  Preparing metadata (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "  Building wheel for openai-whisper (pyproject.toml) ... \u001B[?25l\u001B[?25hdone\n",
      "  Building wheel for lit (setup.py) ... \u001B[?25l\u001B[?25hdone\n"
     ]
    }
   ],
   "source": [
    "! pip install git+https://github.com/openai/whisper.git -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AtAvuKSJxhNw"
   },
   "source": [
    "## Load Whisper speech recognition model\n",
    "Using *large-v2* model which is recommended for best results.\n",
    "\n",
    "Processing 45 minutes of audio takes around 8 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Kr5faKybKi4p"
   },
   "outputs": [],
   "source": [
    "import whisper\n",
    "import torch\n",
    "\n",
    "model = whisper.load_model(\"large-v2\")#, device=\"cuda\")\n",
    "if torch.cuda.is_available():\n",
    "    model = model.to(torch.device(\"cuda\"))\n",
    "model.device\n",
    "# model = whisper.load_model(\"base\", device=\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e200RNNlxn5j"
   },
   "source": [
    "## Check that we are using GPU\n",
    "\n",
    "You should see the output `device(type='cuda', index=0)` below. If you don't, you may be on a CPU-only Colab instance which will run a lot slower. Go to `Runtime->Change Runtime Type` to fix this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u_6_s2iHboR4"
   },
   "outputs": [],
   "source": [
    "model.device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6DcUMDuNGaww"
   },
   "source": [
    "## Transcribe the audio file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q9v8u_o-7XQZ"
   },
   "outputs": [],
   "source": [
    "audio_files = glob(\"drive/MyDrive/lecture_audio_files/*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gyKmKxNr9eqw"
   },
   "outputs": [],
   "source": [
    "transcription_files =  [f\"{Path(audio_file).stem}\" for audio_file in audio_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rm_VvlKQYiNe"
   },
   "outputs": [],
   "source": [
    "language = \"hrv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LZwylWC3-DGP"
   },
   "outputs": [],
   "source": [
    "for audio_file, transcription_file in zip(audio_files, transcription_files):\n",
    "  if Path(f\"drive/MyDrive/lecture_transcriptions/{transcription_file}_{language}.csv\").exists():\n",
    "    print(f\"Skipping {transcription_file}_{language}\")\n",
    "    continue\n",
    "\n",
    "  print(f\"Audio file: {audio_file}\")\n",
    "  print(f\"Transcription file: {transcription_file}_{language}\")\n",
    "  result = model.transcribe(f\"{audio_file}\", verbose=True, language=\"hr\")#, task=\"translate\")\n",
    "  save_transcription(result, f\"{transcription_file}_{language}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W0TokbAoSCty"
   },
   "outputs": [],
   "source": [
    "for file in glob(\"drive/MyDrive/lecture_transcriptions/*.csv\"):\n",
    "  files.download(file)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
